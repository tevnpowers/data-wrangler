{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take Home Project: Wrangling FERC Form 1\n",
    "\n",
    "* This task is an example of the kind of work we do to make public energy data usable for analysis.\n",
    "* We want to be able to explore your general approach together and see how you think about these kinds of problems.\n",
    "* **Spend 2-4 hours working on it.** This doesn't have to happen all at once. We want you to have time to play with the data, step away from it to think, and then come back to it again.\n",
    "* Feel free to use whatever documentation or online resources you would normally consult while working on a data wrangling problem.\n",
    "* Feel free to use additional 3rd party libraries if you want to.  You should be able to install them from within the notebook using `!pip install packagename` or `!conda install packagename`\n",
    "\n",
    "## Email us your notebook within a week.\n",
    "* Send it to [hello@catalyst.coop](mailto:hello@catalyst.coop) (normally we'd have you make a PR but... we don't want everyone looking at each others solutions)\n",
    "* We'll review your notebook and if it looks good, we'll reach out to schedule a longer conversation about it, and another technical interview.\n",
    "\n",
    "## Some questions to keep in mind:\n",
    "* What assumptions are you making about the data?\n",
    "* Is the raw data well structured?\n",
    "* How will you test whether / when those assumptions are valid?\n",
    "* How would you / did you deal with the data that don’t conform to those assumptions?\n",
    "* If there are records which can’t be reasonably cleaned automatically, but were high value in an advocacy context, how would you integrate manual cleaning into the automated process so that the manual effort is captured, and can be incrementally improved over time?\n",
    "* What expectations do you have about the output data?\n",
    "* What kind of data validation checks would you design to make sure that the output meets your expectations? These could be either integrated into the table transformation process, or run on the final output.\n",
    "* How do you decide when data isn’t recoverable?\n",
    "* How will you evaluate the completeness of the data that you’ve been able to extract?\n",
    "* What kind of queries are you trying to make easy with the structure of the output data?\n",
    "* What parts of this process might make sense to generalize / abstract for re-use in extracting, cleaning, and reorganizing data from other tables?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background on the FERC Form 1 Database\n",
    "* The FERC Form 1 collects financial data about electric utilities in the US. It’s a treasure-trove of information if you want to understand how these utilities make and spend money. The capital they have locked up in existing fossil fuel infrastructure is one of the big reasons they fight against the transition to clean energy. Data from the FERC Form 1 can help advocates understand which utilities will be easiest to engage in the transition, and which ones may be hopeless pyromaniacs.\n",
    "* Unfortunately, FERC does not organize its data very well, or do much quality control, so this data is difficult to extract and use. We’ve built a script that pulls together all of FERC’s annual Visual FoxPro databases into a single SQLite database covering all the years of data. Then we write extract and transform functions to pull tables from this multi-year DB and clean them up for easier analysis.\n",
    "* To help us understand how you approach working with messy data and turning it into something usable, we’d like you to develop a strategy for reshaping and cleaning the data in one of these tables.\n",
    "* [Here is some documentation about the FERC Form 1 Database](https://catalystcoop-pudl.readthedocs.io/en/dev/data_sources/ferc1_db_notes.html), including a mapping between database tables and the pages of the PDF that their data is collected from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up access to the FERC Form 1 DB\n",
    "* You can download a copy of our FERC Form 1 SQLite DB from: https://data.catalyst.coop/ferc1.db\n",
    "* Substitute the path to that file on your system below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy as sa\n",
    "import pandas as pd\n",
    "\n",
    "FERC1_DB_PATH = \"../data/ferc1.sqlite\"\n",
    "\n",
    "ferc1_engine = sa.create_engine(f\"sqlite:///{FERC1_DB_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the FERC Form 1 Small Plants table for quantitative analysis.\n",
    "* Explore the Small Plants table (named `f1_gnrt_plant` in the FERC 1 DB).\n",
    "* Refer to the [blank Form 1 (PDF)](https://catalystcoop-pudl.readthedocs.io/en/dev/_downloads/6a316a949a522f595e7575b6fd7034b8/ferc1_blank_2022-11-30.pdf) (pages 410-411) for more context about the table.\n",
    "* Our goal is to make as much of the information as possible available for easy programmatic analysis.\n",
    "* Unfortunately, in its raw form this data is only semi-structured."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify issues in the data\n",
    "* Make a list of issues that would need to be addressed before this table would be ready for analytical use.\n",
    "* Show us how you identified the issues you highlight, and briefly talk through why they're problematic, and how you might approach fixing them.\n",
    "* Don't worry about cataloging every possible issue, but do try to identify several of the biggest problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tackle one or more of the issues you identified\n",
    "* Choose one or more of the major issues you identified above, and address it in Python, using pandas and whatever other packages you find useful.\n",
    "* Imagine these being the first few steps of an ETL pipeline that would ultimately output a tidy, well-structured, analysis-ready database table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the small plants table, ignoring footnote reference columns:\n",
    "small_plants = pd.read_sql(\"f1_gnrt_plant\", ferc1_engine)\n",
    "small_plants = small_plants.loc[:, ~small_plants.columns.str.endswith(\"_f\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_plants.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumption that early data is representative\n",
    "# Assumption that data is well/evenly distributed\n",
    "small_plants.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Issues Identified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraneous Data\n",
    "Many of the rows in the table, from the samples I've looked at, appear to not actually correspond to a single small plant. Some appear to be headings, which introduce a section of small plant rows that follow. While others appear to be an aggregation row, referring to some cumulative information about the rows preceding it. The _yr_constructed_ column appears to be the most reliable indicator of whether a row actually represents a small plant or some extraneous information. However, the rows that do not represent a single row still have useful information that can be used to supplement the rows in the same section that might have missing information. For example for the rows with _respondent_id_ 115, the first row has the value \"Hydro\" in the _plant_name_ column, followed by five rows (2-6), that actually represent small plants, evidenced by the values we can see in those rows, including a valid _yr_constructed_ value. However each plant referenced in those valid rows are missing a value in their _kind_of_fuel_ column. It might be appropriate in this scenario to treat row 1 with the value \"Hydro\" as an introduction row for the small hydro plants that follow. So in cleaning up and transforming this table into a more usable format, I would look for other patterns like this in the data, so that we could use the \"heading\" rows to fill in the gaps of information missing in the valid rows. In this case, we could put \"hydro\" in the _kind_of_fuel_ column for rows 2-6. We see a similar pattern from the same _respondent_id_ 6 section in rows 25-27, where these are likely hydro electric plants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Data and Invalid Data\n",
    "Many of the rows in the table are simply missing values for many of the columns. The short script below looks for columns with missing or invalid data. The output has been copied to the table below for convenience and readability.\n",
    "\n",
    "\n",
    "| Column Name      | Percent of Rows Missing Value |\n",
    "| ----------- | ----------- |\n",
    "| fuel_cost | 89.59% |\n",
    "| expns_fuel   | 83.34% |\n",
    "| kind_of_fuel | 64.84% |\n",
    "| row_prvlg | 58.68% |\n",
    "| operation | 45.11% |\n",
    "| expns_maint | 40.93% |\n",
    "| net_generation | 38.97% |\n",
    "| plant_cost_mw | 37.23% |\n",
    "| plant_cost | 31.09% |\n",
    "| yr_constructed | 29.40% |\n",
    "| net_demand | 4.58% |\n",
    "| capacity_rating | 3.32% |\n",
    "| plant_name | 1.76% |\n",
    "| respondent_id | 0.00% |\n",
    "| report_year | 0.00% |\n",
    "| spplmnt_num | 0.00% |\n",
    "| row_number | 0.00% |\n",
    "| row_seq | 0.00% |\n",
    "| report_prd | 0.00% |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These columns should have non-NaN values based on their Python types.\n",
    "NUMERICAL_COLUMNS = [\n",
    "    'respondent_id',\n",
    "    'report_year',\n",
    "    'spplmnt_num',\n",
    "    'row_number',\n",
    "    'row_seq',\n",
    "    'capacity_rating',\n",
    "    'net_demand',\n",
    "    'net_generation',\n",
    "    'plant_cost',\n",
    "    'plant_cost_mw',\n",
    "    'operation',\n",
    "    'expns_fuel',\n",
    "    'expns_maint',\n",
    "    'fuel_cost',\n",
    "    'report_prd'\n",
    "]\n",
    "\n",
    "# Good indicator of whether a row actually corresponds to a plant\n",
    "# rather than being extraneous information.\n",
    "YEAR_CONSTRUCTED_COL = 'yr_constructed'\n",
    "\n",
    "# Assumption that the value \"None\" is not valid for this column.\n",
    "PLANT_NAME_COL = 'plant_name'\n",
    "PLANT_NAME_NULL = 'none'\n",
    "\n",
    "import re\n",
    "# Simple pattern for identifying valid year entries in \"yr_constructed\" column.\n",
    "# 29.19% of rows report no value in this column.\n",
    "# 0.21% report invalid (via pattern below) values.\n",
    "PATTERN = re.compile(\"^[0-9]{4}$\")\n",
    "\n",
    "def cell_is_invalid(row, column):\n",
    "    # Plant names should not be empty or \"None\"\n",
    "    if column == PLANT_NAME_COL:\n",
    "        plant_name = row[column].strip().lower()\n",
    "        return plant_name == PLANT_NAME_NULL or \\\n",
    "                not plant_name\n",
    "    elif column == YEAR_CONSTRUCTED_COL:\n",
    "        # 29.19% empty values\n",
    "        # 0.21% invalid values\n",
    "        # Year constructed should be a valid\n",
    "        # 4 digit number only\n",
    "        value = row[column].strip() \n",
    "        return not value or not PATTERN.match(value)\n",
    "    elif column in NUMERICAL_COLUMNS:\n",
    "        # For numerical columns we can check if\n",
    "        # the value is NaN \n",
    "        return pd.isna(row[column])\n",
    "    else:\n",
    "        # Catch all, to make sure the value is non-empty\n",
    "        return not row[column].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_missing_columns_report(df):\n",
    "    # Get the number of rows in table so we have\n",
    "    # the denominator when we calculate percentage below.\n",
    "    total = df.shape[0]\n",
    "    \n",
    "    # Dictionary that maps a column name to the computed\n",
    "    # percent of rows missing a valid value for that column. \n",
    "    percent_missing = {}\n",
    "\n",
    "    # For every column, iterate through each row and determine\n",
    "    # if the row has a valid value for the column. Aggregate the\n",
    "    # count of invalid rows and calculate which percent of rows\n",
    "    # are missing a valid value, using the validation function above.\n",
    "    for col in df.columns:\n",
    "        nulls = 0\n",
    "        percent_missing[col] = 0\n",
    "        for index, row in df.iterrows():\n",
    "            if cell_is_invalid(row, col):\n",
    "                nulls += 1\n",
    "\n",
    "        # Percents are initialized as 0, so we only need to update\n",
    "        # here if we found invalid rows.\n",
    "        if nulls > 0:\n",
    "            percent_missing[col] = 100 * nulls / total\n",
    "            \n",
    "    sorted_percents = {k: v for k, v in sorted(percent_missing.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "    for k, v in sorted_percents.items():\n",
    "        print('{:.2f}% of rows missing {} data'.format(v, k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_missing_columns_report(small_plants)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script below continues the exploration of missing data in the table. However, instead of looking through the lens of column values, we take a look at how many rows are missing a valid value for at least one column. This may prove to be less useful than the exploration above, however it's an easy and quick question to answer, so why not.\n",
    "\n",
    "Out of the 19559 total rows in the table, only 381 rows (1.95%) contain a valid value for every column in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rows_with_invalid_data(df):\n",
    "    # Get the number of rows in table so we have\n",
    "    # the denominator when we calculate percentage below.\n",
    "    total = df.shape[0]\n",
    "    \n",
    "    # Tally the number of rows with invalid data\n",
    "    # for at least one column.\n",
    "    missing_row_count = 0\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        # Flag that will flip to true as soon as we identify\n",
    "        # any column with invalid data.\n",
    "        data_missing = False\n",
    "        for col in df.columns:\n",
    "            # If this column is invalid, we can break the loop\n",
    "            # here for the purposes of this exploration, and\n",
    "            # include this row in the tally.\n",
    "            if cell_is_invalid(row, col):\n",
    "                data_missing = True\n",
    "                break\n",
    "        if data_missing:\n",
    "            missing_row_count += 1\n",
    "\n",
    "            \n",
    "    print('{} rows missing at least one valid value.'.format(missing_row_count))\n",
    "    print('{:.2f}% of rows missing at least one valid value.'.format(100 * missing_row_count / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_rows_with_invalid_data(small_plants)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inconsistent/Non-standard Data\n",
    "It's not surprising that the table contains a lot of inconsistent or non-standard data, considering the FERC Form 1 allows groups to provide free text information. For fields like plant names, it's expected for values to be different and even unique in most cases. However, for a field like _kind_of_fuel_ in the table, I believe there's a finite and relatively small set of values that are suitable values to provide here. However, because the form allows for the plants to fill in the free text field, in this table we end up with 109 unique values in this column.\n",
    "\n",
    "Honestly, this is a small enough set that I would create a standardized format for fuel type and manually comb through them to update the table accordingly. For example, something like a pipe-delimited or comma-seperated value that would satisfy describing plants that use any single source of fuel as well as those with combination or hybrid approaches.\n",
    "\n",
    "So if a plant reports using \"Coal/Gas\", I might change that value to \"COAL|GAS\". Similarly any plant that reports \"#2 Oil\" or \"#2\" would end up having identical values in the new created. I am not sure if the numbers are significant here (since there are rows containing \"#6\" or \"No. 6 Oil\") or if it'd only be necessary to note that those plants use oil.\n",
    "\n",
    "As I mentioned, given this table and some more time, I think it'd be simple enough to manually create the mapping of old values to new standardized values in transforming this table. However, if we anticipate needing to run the table transformation frequently and on larger sets of data with minimal manual adjustments, it might be worth it to write a module that can attempt automating the transformations. I've begun to do so in the section below. This single function is far from comprehensive but lays out the vision for the approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List out the unique values in the kind_of_fuel column\n",
    "unique_fuel_values = small_plants['kind_of_fuel'].unique()\n",
    "print('Out of 19559 rows of data, we found {} unique values for the kind_of_fuel column:'.format(len(unique_fuel_values)))\n",
    "for value in unique_fuel_values:\n",
    "    if value.strip():\n",
    "        print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_fuel_type(original_value, new_value, fuel_type, standard):\n",
    "    if fuel_type in original_value:\n",
    "        return new_value + standard\n",
    "    return new_value\n",
    "\n",
    "def standardize_fuel(value):\n",
    "    # Normalize the original value first\n",
    "    value = value.strip().lower()\n",
    "    \n",
    "    # Any of these values signal that we don't\n",
    "    # need to inspect the data any more.\n",
    "    na_list = ['n/a', 'na', 'n.a.']\n",
    "    if value == 'none' or value in na_list or not value:\n",
    "        return 'NONE'\n",
    "\n",
    "    # The chunk below looks for keywords in the original value\n",
    "    # supplied on the form, and creates a new standardized value\n",
    "    # that captures any and all fuel types referenced.\n",
    "    \n",
    "    # Making an assumption that the Oil/Gas # does not matter for now,\n",
    "    # but could easily update the pattern matching if that information\n",
    "    # needs to be captured/preserved. Honestly, I'd default to preserving\n",
    "    # that information but in the interest of time/simplicity, for now I'm\n",
    "    # omitting it.\n",
    "    standardized_fuel = check_fuel_type(value, '', 'oil', 'OIL|')\n",
    "    standardized_fuel = check_fuel_type(value, standardized_fuel, 'gas', 'GAS|')\n",
    "    standardized_fuel = check_fuel_type(value, standardized_fuel, 'diesel', 'DIESEL|')\n",
    "    standardized_fuel = check_fuel_type(value, standardized_fuel, 'hydro', 'HYDRO|')\n",
    "    standardized_fuel = check_fuel_type(value, standardized_fuel, 'coal', 'COAL|')\n",
    "    standardized_fuel = check_fuel_type(value, standardized_fuel, 'water', 'WATER|')\n",
    "    standardized_fuel = check_fuel_type(value, standardized_fuel, 'wind', 'WIND|')\n",
    "    standardized_fuel = check_fuel_type(value, standardized_fuel, 'solar', 'SOLAR|')\n",
    "    standardized_fuel = check_fuel_type(value, standardized_fuel, 'waste heat', 'WASTE HEAT|')\n",
    "    standardized_fuel = check_fuel_type(value, standardized_fuel, 'propane', 'PROPANE|')\n",
    "    standardized_fuel = check_fuel_type(value, standardized_fuel, 'methane', 'METHANE|')\n",
    "    standardized_fuel = check_fuel_type(value, standardized_fuel, 'fossil', 'FOSSIL|')\n",
    "    standardized_fuel = check_fuel_type(value, standardized_fuel, 'river', 'RIVER|')\n",
    "    standardized_fuel = check_fuel_type(value, standardized_fuel, 'rv', 'RV|')\n",
    "    standardized_fuel = check_fuel_type(value, standardized_fuel, 'steam', 'STEAM|')\n",
    "    return standardized_fuel\n",
    "\n",
    "def standardize_values(df):\n",
    "    fuel_values = df['kind_of_fuel'].unique()\n",
    "    print('Original\\tStandardized')\n",
    "    for value in fuel_values:\n",
    "        standardized_value = standardize_fuel(value)\n",
    "        print('{}\\t{}'.format(value, standardized_value))\n",
    "        \n",
    "# Below you can see a side by side comparison of original form values\n",
    "# and what the updated standardized value would be ()\n",
    "standardize_values(small_plants)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the data\n",
    "### Standardize Values\n",
    "The first effort in the cleaning process is to standardize values in the columns where it would be useful. As mentioned above, I outlined _kind_of_fuel_ as a column which could likely be standardized. So in the code snippet below, we use that standardization process to update those values in our data frame.\n",
    "\n",
    "### Remove Extraneous Rows\n",
    "Secondly, I want to remove rows that aren't useful for the analysis which I'm assuming consumers of this table would want to conduct. In the case of this table, I noted earlier that the rows of most significance appear to be those with a valid _yr_constructed_ value. Those seem to be the rows which actually correspond to a small plant, and the rows which our consumers are likely most interested in. However, I also noted that some of the extraneous rows do contain important information that would be useful to apply to the small plant rows which might be missing some values. If this were a real exercise, before removing extraneous rows, I would salvage the useful data from them, supplement the real rows that we're keeping with the salvaged data (e.g. removing a row that introduces a set of \"Hydro\" plants, but making sure that the corresponding plant rows contain \"hydro\" in their _kind_of_fuel_ column). Given the short time window, I'm simply removing these rows for not as a prototype. Inline comments describe the same decision/thought process as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consts PATTERN and YEAR_CONSTRUCTED_COL come from the\n",
    "# missing data/invalid data section above. Copied here for convenience:\n",
    "# YEAR_CONSTRUCTED_COL = 'yr_constructed'\n",
    "# PATTERN = re.compile(\"^[0-9]{4}$\")\n",
    "\n",
    "def row_represents_plant(row):\n",
    "    year_constructed = row[YEAR_CONSTRUCTED_COL].strip()\n",
    "    return PATTERN.match(year_constructed)\n",
    "    \n",
    "def clean_data_frame(df):\n",
    "    indexes_for_deletion = []\n",
    "    for index, row in df.iterrows():\n",
    "        # Mark rows with invalid years for deletion\n",
    "        # NOTE: In the interest of staying within the 2-4 hour window for the assignment,\n",
    "        # I did not have time to complete the proposal I noted above in the \"Extraneous Data\"\n",
    "        # section. The proposal was to look for rows that act as a heading for rows that\n",
    "        # follow it, where those heading rows often contain information about the fuel type\n",
    "        # for rows that follow it. This script simply removes all invalid rows to clean up\n",
    "        # the table for usability, however, given more time and in the real world, I'd salvage\n",
    "        # information from those rows before discarding.\n",
    "        if not row_represents_plant(row):\n",
    "            indexes_for_deletion.append(index)\n",
    "        \n",
    "        # Standardadize fuel values\n",
    "        df.at[index,'kind_of_fuel'] = standardize_fuel(df.at[index,'kind_of_fuel'])\n",
    "  \n",
    "    df = df.drop(df.index[indexes_for_deletion])\n",
    "    return df\n",
    "\n",
    "small_plants_copy = small_plants.copy()\n",
    "small_plants_copy = clean_data_frame(small_plants_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "small_plants_copy.head(60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
